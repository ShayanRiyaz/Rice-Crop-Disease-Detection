{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Rice Crop Disease Detection using TensorFlow\n\n## Table of Contents\n- [Importing Libraries](#ImportingLibraries)\n- [Loading Dataset](#LoadingDataset)\n- [Resizing Image](#Resize)\n- [Split into Training and Validation](#Split)\n- [Image Count](#ImageCount)\n- [Viewing Images](#ViewingImages)\n    - [BrownSpot](#BrownSpot)\n    - [Healthy](#Healthy)\n    - [Hispa](#Hispa)\n    - [LeafBlast](#LeafBlast)\n- [Data Augmentation and Generators](#DataAugAndGen)\n- [Callback](#Callback)\n- [Models](#Models)\n    - [1. Model - Conv2D](#Conv2D)\n        - [Metrics](#MetricsConv2D)\n        - [Observing the Convolutions](#ObservingConv2D)\n    - [2. Model - InceptionV3](#InceptionV3)\n        - [Metrics](#MetricInceptionv3)\n    - [3. Model - EfficientNet](#EfficientNet)\n        - [Metrics](#MetricsEfficientv2)\n- [Export as TensorFlow LITE](#TFLITE)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries <a name=\"ImportingLibraries\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport tensorflow as tf\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nimport tensorflow_hub as hub\nimport os\n\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\nfrom keras import optimizers\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Version \", tf.__version__)\nprint(\"Eager mode:\", tf.executing_eagerly())\nprint(\"Hub version: \", hub.__version__)\nprint(\"GPU is\",\"available\" if tf.test.is_gpu_available() else\"Not Available\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Dataset <a name=\"LoadingDataset\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#!/usr/bin/python\n\nimport os, sys\n\n# Create new Train and val folders\n\nbase_dir = 'kaggle/input/RiceLeafs'\ntrain_path = '/kaggle/input/RiceLeafs/train'\nval_path = 'kaggle/input/RiceLeafs/validation/'\n\ncolumn_names = os.listdir(train_path)\nfor i in column_names:\n    os.makedirs(f'../kaggle/output/train/{i}')\n    os.makedirs(f'../kaggle/output/validation/{i}')\n\nout_path = '../kaggle/output/train/'\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Resizing Image <a name=\"Resize\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\ndef resize(input_path,folder,column_name):\n    dirs = os.listdir(input_path)\n    for item in dirs:\n        item_path = input_path +'/' +item\n        if os.path.isfile(item_path):\n            #print('CHECK')\n            im = Image.open(item_path)\n\n            # Check whether the specified \n            # path exists or not \n            outpath = f'/kaggle/kaggle/output/{folder}/{column_name}'\n            temp_out_path = outpath+'/'+item\n            f, e = os.path.splitext(temp_out_path)\n\n            imResize = im.resize((255,255), Image.ANTIALIAS)\n            #print('CHECK 3')\n            imResize.save(f + '.jpg', 'JPEG', quality=90)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_path = '../input/RiceLeafs/train/Healthy'\nfolder = 'train'\ncolumn_name = 'Healthy'\nresize(input_path,folder,column_name)\n\ninput_path = '../input/RiceLeafs/train/BrownSpot'\nfolder = 'train'\ncolumn_name = 'BrownSpot'\nresize(input_path,folder,column_name)\n\ninput_path = '../input/RiceLeafs/train/Hispa'\nfolder = 'train'\ncolumn_name = 'Hispa'\nresize(input_path,folder,column_name)\n\ninput_path = '../input/RiceLeafs/train/LeafBlast'\nfolder = 'train'\ncolumn_name = 'LeafBlast'\nresize(input_path,folder,column_name)\n\nprint('Done with train resizing')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## VALIDATION\ninput_path = '../input/RiceLeafs/validation/Healthy'\nfolder = 'validation'\ncolumn_name = 'Healthy'\nresize(input_path,folder,column_name)\n\ninput_path = '../input/RiceLeafs/validation/BrownSpot'\nfolder = 'validation'\ncolumn_name = 'BrownSpot'\nresize(input_path,folder,column_name)\n\ninput_path = '../input/RiceLeafs/validation/Hispa'\nfolder = 'validation'\ncolumn_name = 'Hispa'\nresize(input_path,folder,column_name)\n\ninput_path = '../input/RiceLeafs/validation/LeafBlast'\nfolder = 'validation'\ncolumn_name = 'LeafBlast'\nresize(input_path,folder,column_name)\n\nprint('Done with Validation resizing')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.path.exists('/kaggle/kaggle/output/validation/Healthy/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.path.exists('/kaggle/kaggle/output/train/')\nos.path.exists('/kaggle/kaggle/output/validation/')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('/kaggle/kaggle/output/train/BrownSpot/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = os.path.join(os.path.dirname('/kaggle/kaggle/'), 'output')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split into Training and Validation  <a name=\"Split\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use this if you avoided the resizing\ndata_dir = os.path.join(os.path.dirname('/output/'), 'RiceLeafs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir = os.path.join(data_dir, 'train')\ntrain_BrownSpot_dir = os.path.join(train_dir, 'BrownSpot')\ntrain_Healthy_dir = os.path.join(train_dir, 'Healthy')\ntrain_Hispa_dir = os.path.join(train_dir, 'Hispa')\ntrain_LeafBlast_dir = os.path.join(train_dir, 'LeafBlast')\n\n\nvalidation_dir = os.path.join(data_dir, 'validation')\nvalidation_BrownSpot_dir = os.path.join(validation_dir, 'BrownSpot')\nvalidation_Healthy_dir = os.path.join(validation_dir, 'Healthy')\nvalidation_Hispa_dir = os.path.join(validation_dir, 'Hispa')\nvalidation_LeafBlast_dir = os.path.join(validation_dir, 'LeafBlast')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_BrownSpot_names = os.listdir(train_BrownSpot_dir)\nprint(train_BrownSpot_names[:10])\n\ntrain_Healthy_names =  os.listdir(train_Healthy_dir)\nprint(train_Healthy_names[:10])\n\ntrain_Hispa_names = os.listdir(train_Hispa_dir)\nprint(train_Hispa_names[:10])\n\ntrain_LeafBlast_names =  os.listdir(train_LeafBlast_dir)\nprint(train_LeafBlast_names[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Image Count <a name=\"ImageCount\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport time\nimport os\nfrom os.path import exists\n\ndef count(dir, counter=0):\n    \"returns number of files in dir and subdirs\"\n    for pack in os.walk(dir):\n        for f in pack[2]:\n            counter += 1\n    return dir + \" : \" + str(counter) + \" files\"\n\nprint('total images for training :', count(train_dir))\nprint('total images for validation :', count(validation_dir))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Viewing Images  <a name=\"ViewingImages\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n\n# Parameters for our graph; we'll outpu images in a 4x4 configuration\nnrows = 4\nncols = 4\n\n# for iternating over images\npic_index = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BrownSpot <a name=\"BrownSpot\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up matplotlib fig, and size it to fit 4x4 pics\n\nfig = plt.gcf()\nfig.set_size_inches(ncols *4, nrows*4)\n\npic_index += 8\nnext_BrownSpot_pix = [os.path.join(train_BrownSpot_dir, fname)\n                for fname in train_BrownSpot_names[pic_index-8:pic_index]]\nfor i, img_path in enumerate(next_BrownSpot_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows,ncols,i +1)\n  #sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Healthy <a name=\"Healthy\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up matplotlib fig, and size it to fit 4x4 pics\n\nfig = plt.gcf()\nfig.set_size_inches(ncols *4, nrows*4)\n\npic_index += 8\nnext_Healthy_pix = [os.path.join(train_Healthy_dir, fname)\n                for fname in train_Healthy_names[pic_index-8:pic_index]]\n\n\nfor i, img_path in enumerate(next_Healthy_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows,ncols,i +1)\n  #sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hispa <a name=\"Hispa\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up matplotlib fig, and size it to fit 4x4 pics\n\nfig = plt.gcf()\nfig.set_size_inches(ncols *4, nrows*4)\n\npic_index += 8\n\nnext_Hispa_pix = [os.path.join(train_Hispa_dir, fname)\n                for fname in train_Hispa_names[pic_index-8:pic_index]]\n\n\nfor i, img_path in enumerate(next_Hispa_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows,ncols,i +1)\n  #sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LeafBlast <a name=\"LeafBlast\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up matplotlib fig, and size it to fit 4x4 pics\n\nfig = plt.gcf()\nfig.set_size_inches(ncols *4, nrows*4)\n\npic_index += 8\n\nnext_LeafBlast_pix = [os.path.join(train_LeafBlast_dir, fname)\n                for fname in train_LeafBlast_names[pic_index-8:pic_index]]\n\nfor i, img_path in enumerate(next_LeafBlast_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows,ncols,i +1)\n  #sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Augmentation and Generators <a name=\"DataAugAndGen\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_SHAPE = (244, 244)\nBATCH_SIZE = 64 #@param {type:\"integer\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inputs are suitably resized for the selected module. Dataset augmentation (i.e., random distortions of an image each time it is read) improves training, esp. when fine-tuning.\n\nvalidation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\nvalidation_generator = validation_datagen.flow_from_directory(\n    validation_dir, \n    shuffle=False, \n    seed=42,\n    color_mode=\"rgb\", \n    class_mode=\"categorical\",\n    target_size=IMAGE_SHAPE,\n    batch_size=BATCH_SIZE)\n\ndo_data_augmentation = True #@param {type:\"boolean\"}\nif do_data_augmentation:\n  train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n      rescale = 1./255,\n      rotation_range=40,\n      horizontal_flip=True,\n      width_shift_range=0.2, \n      height_shift_range=0.2,\n      shear_range=0.2, \n      zoom_range=0.2,\n      fill_mode='nearest' )\nelse:\n  train_datagen = validation_datagen\n  \ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,  \n    shuffle=True, \n    seed=42,\n    color_mode=\"rgb\", \n    class_mode=\"categorical\",\n    target_size=IMAGE_SHAPE,\n    batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_generator.num_classes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Callback <a name=\"Callback\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self,epoch,log = {}):\n    if(log.get('accuracy')> 0.99):\n      if(log.get('val_accuracy')>0.99):\n        print(\"\\n Reached 99% Accuracy for both train and val.\")\n        self.model.stop_training = True\n\ncallbacks = MyCallback()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models <a name=\"Model\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1. Model - Conv2D <a name=\"Conv2D\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(16,(3,3),activation = 'relu',input_shape = (244,244,3)),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(32,(3,3),activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(64,(3,3),activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128,activation = 'relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(256,activation = 'relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(4,activation = 'softmax')\n\n],    name = 'Conv2D_Model')\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LEARNING_RATE = 0.001 #@param {type:\"number\"}\n\nmodel.compile(optimizer = tf.keras.optimizers.Adam(),\n              loss = 'categorical_crossentropy',\n              metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS=10 #@param {type:\"integer\"}\n\nhistory = model.fit_generator(\n        train_generator,\n        steps_per_epoch=train_generator.samples//train_generator.batch_size,\n        epochs=EPOCHS,\n        validation_data=validation_generator,\n        callbacks = [callbacks],\n        validation_steps=validation_generator.samples//validation_generator.batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Metrics <a name=\"MetricsConv2D\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pylab as plt\nimport numpy as np\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(EPOCHS)\n\nplt.figure(figsize=(20, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\nplt.ylabel(\"Accuracy (training and validation)\")\nplt.xlabel(\"Training Steps\")\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.ylabel(\"Loss (training and validation)\")\nplt.xlabel(\"Training Steps\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observing the Convolutions  <a name=\"ObservingConv2D\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport random\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\n\n\n# Lets define a new Model that will take an image as an input and will output \n# the intermediate representations for all layers in the previous model after \n# the first\n\nsuccessive_outputs = [layer.output for layer in model.layers[1:]]\n\n# Visualization_model = Model(img_input,successive_outputs)\nvisualization_model = tf.keras.models.Model(inputs = model.input,\n                                            outputs = successive_outputs)\n\n# Lets prepare a random input image form the training set.\n\nBrownSpot_img_files = [os.path.join(train_BrownSpot_dir, f) for f in train_BrownSpot_names]\nHealthy_files = [os.path.join(train_Healthy_dir, f) for f in train_Healthy_names]\nimg_path = random.choice(BrownSpot_img_files + Healthy_files)\n\n\nimg = load_img(img_path,target_size = (244,244)) # This is a PIL image\nx = img_to_array(img)  # Numpy array with shape (244,244,3)\nx = x.reshape((1,) + x.shape) # Numpy array with shape (1,244,244,3)\n\n# Rescale by 1/255\nx /=255\n\n\n# Let's run our image through our network, thus obtaining all\n# Intermediate representations for this image.\nsuccessive_feature_maps = visualization_model.predict(x)\n\n# These are the names of the layers so we can have them as part of our plot\nlayer_names = [layer.name for layer in model.layers[1:]]\n\n\n# Now lets display our representations\nfor layer_name, feature_map in zip(layer_names, successive_feature_maps):\n  if len(feature_map.shape) == 4:\n    # Just do this for the conv/maxpool layers, for the fully-connected layers\n    n_features = feature_map.shape[-1] # number of features in feature map\n    # The feature map has shape (1,size,size,n_features)\n    size = feature_map.shape[1]\n    # We will title our images in this matrix\n    display_grid = np.zeros((size, size* n_features))\n    for i in range(n_features):\n      # Post process the feature to make it visibly palatable\n      x = feature_map[0,:,:,i]\n      x -= x.mean()\n      x /= x.std()\n      x *= 64\n      x+= 128\n      x = np.clip(x,0,255).astype('uint8')\n      # We'll tile each filter into this big horizontal grid\n      display_grid[:,i*size:(i+1)*size] = x\n    # Display the grid\n    scale = 20. / n_features\n    plt.figure(figsize=(scale*n_features,scale))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid,aspect = 'auto', cmap = 'viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.Model - Inception <a name=\"InceptionV3\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Downloading Weights <a name=\"DownloadWeights\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget --no-check-certificate \\\n    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n    -O /tmp/inception_v3_weights_tf.dim_ordering_tf_kernels.notop.h5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nlocal_weights_file = '/tmp/inception_v3_weights_tf.dim_ordering_tf_kernels.notop.h5'\n\npre_trained_model = InceptionV3(\n                                input_shape = (244,244,3),\n                                include_top= False,\n                                weights = None\n)\n\npre_trained_model.load_weights(local_weights_file)\n\nfor layer in pre_trained_model.layers:\n    layer.trainable = False\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_layer = pre_trained_model.get_layer('mixed7')\nprint(f'The shape of the last layer is {last_layer.output_shape}')\noutput_layer = last_layer.output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\nx = tf.keras.layers.Flatten()(output_layer)\nx = tf.keras.layers.Dense(512, activation='relu')(x)\nx = tf.keras.layers.Dropout(0.4)(x)\n#x = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense(4, activation='softmax')(x)\n\nmodel = Model(pre_trained_model.input, x,name=\"RiceLeafs_Inception_model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LEARNING_RATE = 0.001 #@param {type:\"number\"}\n\nmodel.compile(optimizer = tf.keras.optimizers.RMSprop(lr = LEARNING_RATE),\n              loss = 'categorical_crossentropy',\n              metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS=10 #@param {type:\"integer\"}\n\nhistory = model.fit_generator(\n        train_generator,\n        steps_per_epoch=train_generator.samples//train_generator.batch_size,\n        epochs=EPOCHS,\n        validation_data=validation_generator,\n        callbacks = [callbacks],\n        validation_steps=validation_generator.samples//validation_generator.batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Metrics <a name=\"MetricsInceptionv3\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pylab as plt\nimport numpy as np\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(EPOCHS)\n\nplt.figure(figsize=(20, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\nplt.ylabel(\"Accuracy (training and validation)\")\nplt.xlabel(\"Training Steps\")\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.ylabel(\"Loss (training and validation)\")\nplt.xlabel(\"Training Steps\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Model - EfficientNet v2 <a name=\"EfficientNet\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### TensorFlow Hub Dataset\n- [EfficientNet B7](https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nmodel = tf.keras.Sequential([\nhub.KerasLayer(\"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\"),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(512, activation='relu'),\n\n  tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n])\n\nmodel.build([None, 244, 244, 3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compile model specifying the optimizer learning rate\n\nLEARNING_RATE = 0.0001 #@param {type:\"number\"}\n\nmodel.compile(\n   optimizer=tf.keras.optimizers.Adam(lr=LEARNING_RATE), \n   loss='categorical_crossentropy',\n   metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS=10 #@param {type:\"integer\"}\n\nhistory = model.fit_generator(\n        train_generator,\n        steps_per_epoch=train_generator.samples//train_generator.batch_size,\n        epochs=EPOCHS,\n        validation_data=validation_generator,\n        #callbacks = [callbacks],\n        validation_steps=validation_generator.samples//validation_generator.batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Metrics <a name=\"MetricsEfficientv2\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pylab as plt\nimport numpy as np\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(EPOCHS)\n\nplt.figure(figsize=(20, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\nplt.ylabel(\"Accuracy (training and validation)\")\nplt.xlabel(\"Training Steps\")\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.ylabel(\"Loss (training and validation)\")\nplt.xlabel(\"Training Steps\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_SHAPE[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import OpenCV\nimport cv2\n\n# Utility\nimport itertools\nimport random\nfrom collections import Counter\nfrom glob import iglob\n\n\ndef load_image(filename):\n    img = cv2.imread(os.path.join(data_dir, validation_dir, filename))\n    img = cv2.resize(img,(IMAGE_SHAPE[0], IMAGE_SHAPE[1]) )\n    img = img /255\n    \n    return img\n\n\ndef predict(image):\n    probabilities = model.predict(np.asarray([img]))[0]\n    class_idx = np.argmax(probabilities)\n    \n    return {classes[class_idx]: probabilities[class_idx]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, filename in enumerate(random.sample(validation_generator.filenames, 5)):\n    print(\"SOURCE: class: %s, file: %s\" % (os.path.split(filename)[0], filename))\n    \n    img = load_image(filename)\n    prediction = predict(img)\n    print(\"PREDICTED: class: %s, confidence: %f\" % (list(prediction.keys())[0], list(prediction.values())[0]))\n    plt.imshow(img)\n    plt.figure(idx)    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Export as TensorFlowLITE <a name=\"TFLITE\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nt = time.time()\n\nexport_path = \"/tmp/saved_models/{}\".format(int(t))\ntf.keras.experimental.export_saved_model(model, export_path)\n\nexport_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now confirm that we can reload it, and it still gives the same results\nreloaded = tf.keras.experimental.load_from_saved_model(export_path, custom_objects={'KerasLayer':hub.KerasLayer}) # custom_objects depends on model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert the model to TFLite\n!mkdir \"tflite_models\"\nTFLITE_MODEL = \"tflite_models/rice_leaf_disease.tflite\"\n\n\n# Get the concrete function from the Keras model.\nrun_model = tf.function(lambda x : reloaded(x))\n\n# Save the concrete function.\nconcrete_func = run_model.get_concrete_function(\n    tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype)\n)\n\n# Convert the model to standard TensorFlow Lite model\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\nconverted_tflite_model = converter.convert()\nopen(TFLITE_MODEL, \"wb\").write(converted_tflite_model)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}